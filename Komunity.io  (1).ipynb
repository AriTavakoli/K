{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    "Create the webscraper for Toptal\n",
    "\n",
    "Step 2:\n",
    "\n",
    "Scrape the user profile data\n",
    "\n",
    "Step 3:\n",
    "\n",
    "Package the data into Database format\n",
    "\n",
    "Step 4:\n",
    "\n",
    "make the database \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "All the descriptions:\n",
    "- Profile_description\n",
    "- Employment_history\n",
    "- Expertise\n",
    "- Education\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to scrape all the links for the different types of developers\n",
    "\n",
    "class Webscraper_developers():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "\n",
    "    DRIVER_PATH = '/Users/kourosht/Downloads/chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=DRIVER_PATH, options = options)\n",
    "    driver.get('https://www.toptal.com/developers/all')\n",
    "    Types_of_developers = driver.find_elements_by_tag_name(\"a\")\n",
    "    links = []\n",
    "    for i in Types_of_developers:\n",
    "        href = i.get_attribute('href')\n",
    "        if href is not None:\n",
    "            links.append(href)\n",
    "    Developer_links = links[9:948]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "\n",
    "DRIVER_PATH = '/Users/kourosht/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH, options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To create the list on link-names to further scrape information\n",
    "class name_links:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "\n",
    "    DRIVER_PATH = '/Users/kourosht/Downloads/chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=DRIVER_PATH, options = options)\n",
    "    driver.get('https://www.toptal.com/artificial-intelligence')\n",
    "    section = driver.find_elements_by_class_name('DZ2wWD8B')   \n",
    "    links = []\n",
    "    for i in section:\n",
    "        links.append(i.find_element_by_tag_name('a').get_attribute('href'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.toptal.com/resume/abhimanyu-veer-aditya',\n",
       " 'https://www.toptal.com/resume/vince-jankovics',\n",
       " 'https://www.toptal.com/resume/dan-napierski',\n",
       " 'https://www.toptal.com/resume/dilip-mathew-thomas',\n",
       " 'https://www.toptal.com/resume/rajeev-gupta',\n",
       " 'https://www.toptal.com/resume/miguel-duarte',\n",
       " 'https://www.toptal.com/resume/goran-trlin',\n",
       " 'https://www.toptal.com/resume/josip-bojcic',\n",
       " 'https://www.toptal.com/resume/nenad-spuzic']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_links.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main webscraper that takes all the information from a user's profile \n",
    "class Ai:\n",
    "    driver.get('https://www.toptal.com/resume/abhimanyu-veer-aditya')\n",
    "    \n",
    "    name = []\n",
    "    short_description = []\n",
    "    bio = []\n",
    "    availability = []\n",
    "    preferred_environment = [] \n",
    "    portfolio_and_experience = []\n",
    "    portfolio = []\n",
    "    experience = [] \n",
    "    employment_title = []\n",
    "    employment_company = [] \n",
    "    employment_dates =[]\n",
    "    employment_technologies = []\n",
    "    main_experience = []\n",
    "    date_index = []\n",
    "    techy = []\n",
    "    \n",
    "    \n",
    "    types = driver.find_elements_by_class_name('resume_top-tags')\n",
    "    \n",
    "    \n",
    "    # Name \n",
    "    name.append(driver.find_element_by_class_name('resume_top-info_name').text)\n",
    "    #Short description\n",
    "    short_description.append(driver.find_element_by_class_name('resume_top-info_short_description').text)\n",
    "    # Bio\n",
    "    bio.append(driver.find_element_by_class_name('resume_top-info_bio').text)\n",
    "    # Portfolio and Experience \n",
    "    portfolio_sec = driver.find_elements_by_class_name('resume_details-list_item')\n",
    "    \n",
    "    for i in portfolio_sec:\n",
    "        portfolio_and_experience.append(i.text)\n",
    "    \n",
    "    splitting_portfolio_and_experience = [i.splitlines() for i in portfolio_and_experience]\n",
    "    # Portfolio and Experience ----- Lists\n",
    "    portfolio.append(splitting_portfolio_and_experience[0:3])\n",
    "    experience.append(splitting_portfolio_and_experience[3:])\n",
    "    \n",
    "    \n",
    "    # Availability \n",
    "    availability = driver.find_element_by_xpath('//*[@id=\"Availability\"]/div/div/div[1]/div/div/h2').text\n",
    "    # Preferred_envioronment \n",
    "    preferred_environment.append(driver.find_element_by_xpath('//*[@id=\"Availability\"]/div/div/div[2]/div/div/div').text)\n",
    "    \n",
    "    \n",
    "    # Employment\n",
    "    employment_title1 = driver.find_elements_by_class_name('resume_section-content')\n",
    "    for i in employment_title1:\n",
    "        employment_title.append(i.text)\n",
    "        \n",
    "    parse = employment_title[0]\n",
    "\n",
    "    parsed = parse.splitlines()\n",
    "     \n",
    "    \n",
    "    for i in parsed:\n",
    "        if \"Technologies:\" in i:\n",
    "            techy.append(parsed.index(i))\n",
    "    \n",
    "    date_index = []\n",
    "    for i in parsed:\n",
    "        if re.match('[0-9/-]',i):\n",
    "            date_index.append(parsed.index(i))\n",
    "   \n",
    "    for i,t in zip(techy,date_index):\n",
    "        main_experience.append(parsed[t-1:i+1])\n",
    "        \n",
    "    \n",
    "\n",
    "    #employment_company.append(driver.find_element_by_xpath(''))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #portfolio_sec1 = portfolio_sec.find_elements_by_tag_name('a')                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self Employed',\n",
       " '2018 - PRESENT',\n",
       " 'Independent Machine Learning Consultant',\n",
       " 'Built and deployed multiple personalization ML pipelines to lift offer/coupon conversion rate for customers of major restaurant chain. Application built on Azure Databricks platform with business-configurable pipelines for training, tuning, testing and prediction, using Pandas (Python), Spark (PySpark), sklearn and Spark ML. Deployment to production using Azure Data Factory.',\n",
       " 'Automated, hardened, and deployed multiple ML pipelines on AWS (Elastic Map Reduce with Spark and Lambda) to predict next-best-action, forecast performance and predict/prevent churn for sales representatives of major corporation. Data processing used Python, PySpark and Spark SQL. ML models built using Microsoft ML for Spark.',\n",
       " 'Advised a mid-stage startup on the requirements, features, and architecture needed to support ML pipelines in their high-speed stream processing framework and in-memory data grid. Worked directly with the CEO/CTO and senior technical team.',\n",
       " 'Technologies: Optimization, AWS S3, AWS EC2, Docker, Python, Pandas, Scikit-learn, XGBoost, Solutions Architecture, System Architecture']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(Ai.name)\n",
    "#print(Ai.short_description)\n",
    "#print(Ai.bio)\n",
    "#print(Ai.preferred_environment)\n",
    "\n",
    "#print(Ai.portfolio)\n",
    "#print(Ai.experience)\n",
    "\n",
    "#print(Ai.employment_title)\n",
    "#print(Ai.employment_company)\n",
    "#print(Ai.employment_dates)\n",
    "#print(Ai.employment_technologies)\n",
    "\n",
    "# Portfolio and Experience \n",
    "\n",
    "parse = Ai.employment_title[0]\n",
    "\n",
    "parsed = parse.splitlines()\n",
    "\n",
    "\n",
    "\n",
    "Ai.main_experience[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = []\n",
    "for i in parsed:\n",
    "    if re.match('[0-9/-]',i):\n",
    "        date_index.append(parsed.index(i))\n",
    "main_experience = []        \n",
    "for i,t in zip(techy,date_index):\n",
    "    main_experience.append(parsed[t-1:i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 13, 23, 30, 37]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = []\n",
    "# date = []\n",
    "# company = []\n",
    "# tech = []\n",
    "# experience = []\n",
    "# index = []\n",
    "\n",
    "# pattern = re.compile('[0-9/-]')\n",
    "\n",
    "# for i in parsed:\n",
    "# #Date \n",
    "#     if re.match('[0-9/-]',i):\n",
    "#         date.append(i)\n",
    "#         index.append(parsed.index(i))\n",
    "# # experience \n",
    "#     elif len(i) >50:\n",
    "#         experience.append(i)   \n",
    "# #technologies \n",
    "#     elif \"Technologies:\" in i:\n",
    "#         tech.append(i)\n",
    "        \n",
    "# # Title         \n",
    "# for i in index:\n",
    "#     title.append(parsed[i-1])\n",
    "        \n",
    "        \n",
    "    \n",
    "#     #Technologies\n",
    "  \n",
    "        \n",
    "# #for i,line in enumerate(parsed):\n",
    "#    # print(i,line)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "techy = []\n",
    "for i in parsed:\n",
    "    if \"Technologies:\" in i:\n",
    "        techy.append(parsed.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = []\n",
    "title = []\n",
    "Name = []\n",
    "description = []\n",
    "def Get_names(link):\n",
    "    \n",
    "    driver.get(link)\n",
    "    section = driver.find_elements_by_class_name('_2TE-s1I6') \n",
    "    person = driver.find_elements_by_class_name('_17uNnShe')\n",
    "    persons_des = driver.find_elements_by_class_name('_2B3RoUMO')\n",
    "    \n",
    "    for i in person:\n",
    "        title.append(i.find_element_by_tag_name('p').text)\n",
    "        Name.append(i.find_element_by_tag_name('a').text)\n",
    "        description.append(i.find_element_by_class_name('_2c0BPn8U').text)\n",
    "#i.find_element_by_tag_name('n5whUZd').text\n",
    "        \n",
    "    for i in persons_des:\n",
    "        location.append(i.find_element_by_class_name('_3t5p5yhQ').text)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webscraper_developers.Developer_links\n",
    "for i in Webscraper_developers.Developer_links:\n",
    "    Get_names(i)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8fbb52d8f50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'title' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(title))\n",
    "print(len(location))\n",
    "print(len(Name))\n",
    "print(len(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f004b820f058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdecription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'title' is not defined"
     ]
    }
   ],
   "source": [
    "title.clear()\n",
    "location.clear()\n",
    "Name.clean()\n",
    "decription.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Names'] = Name\n",
    "df['Description'] = description\n",
    "df['Location'] = location\n",
    "df['Title'] = title\n",
    "df.to_csv('ToptalComplete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "writer = pd.ExcelWriter('ToptalMain.xlsx')\n",
    "\n",
    "df.to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_links = Webscraper_developers.Developer_links[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thanks = {}\n",
    "name = ['1T','2T','3T','4T']\n",
    "one = 'one'\n",
    "two = 'two'\n",
    "\n",
    "\n",
    "for i in name: \n",
    "    thanks[i] = {one:two}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this= [1,2,3]\n",
    "\n",
    "def Add(yup):\n",
    "    return yup +1\n",
    "\n",
    "hehe = list(map(Add2.Add1,this))\n",
    "hehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add2():\n",
    "    def Add1(yup):\n",
    "        return yup +1\n",
    "\n",
    "hehe = list(map(Add2.Add1,this))\n",
    "hehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scrape_Developer_links.ai_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ai_Engineers = dict(zip(Scrape_Developer_links.ai_names,Scrape_Developer_links.ai_profiles))\n",
    "Ai_Engineers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
