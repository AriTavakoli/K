{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    "Create the webscraper for Toptal\n",
    "\n",
    "Step 2:\n",
    "\n",
    "Scrape the user profile data\n",
    "\n",
    "Step 3:\n",
    "\n",
    "Package the data into Database format\n",
    "\n",
    "Step 4:\n",
    "\n",
    "make the database \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "All the descriptions:\n",
    "- Profile_description\n",
    "- Employment_history\n",
    "- Expertise\n",
    "- Education\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to scrape all the links for the different types of developers\n",
    "\n",
    "class Webscraper_developers():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "\n",
    "    DRIVER_PATH = '/Users/kourosht/Downloads/chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=DRIVER_PATH, options = options)\n",
    "    driver.get('https://www.toptal.com/developers/all')\n",
    "    Types_of_developers = driver.find_elements_by_tag_name(\"a\")\n",
    "    links = []\n",
    "    for i in Types_of_developers:\n",
    "        href = i.get_attribute('href')\n",
    "        if href is not None:\n",
    "            links.append(href)\n",
    "    Developer_links = links[9:948]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "\n",
    "DRIVER_PATH = '/Users/kourosht/Downloads/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH, options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To create the list on link-names to further scrape information\n",
    "class name_links:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "\n",
    "    DRIVER_PATH = '/Users/kourosht/Downloads/chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=DRIVER_PATH, options = options)\n",
    "    driver.get('https://www.toptal.com/artificial-intelligence')\n",
    "    section = driver.find_elements_by_class_name('DZ2wWD8B')   \n",
    "    links = []\n",
    "    for i in section:\n",
    "        links.append(i.find_element_by_tag_name('a').get_attribute('href'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in name_links.links:\n",
    "    Scraper(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Technical Associate',\n",
       " '2005 - 2007',\n",
       " 'Trilogy',\n",
       " \"Designed and developed the software for Trilogy's email marketing service for, used by clients such as Gateway and Orbitz. The software used segmentation and association rule mining to increase sales, margins, and engagement (email opens and clicks), and integrated data such as demographic, email activity, clickstream, promotional, etc.\",\n",
       " 'Executed weekly campaigns that generated millions of targeted emails, measured lift through A/B testing and reported results in the form of pivot tables and dashboards.',\n",
       " 'Technologies: Microsoft SQL Server, Subversion (SVN), Microsoft, Java']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_experience[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main webscraper that takes all the information from a user's profile \n",
    "name = []\n",
    "short_description = []\n",
    "bio = []\n",
    "availability = []\n",
    "preferred_environment = [] \n",
    "portfolio_and_experience = []\n",
    "portfolio = []\n",
    "experience = [] \n",
    "employment_title = []\n",
    "employment_company = [] \n",
    "employment_dates =[]\n",
    "employment_technologies = []\n",
    "main_experience = []\n",
    "date_index = []\n",
    "techy = []\n",
    "languages = [] \n",
    "Libraries_APIs = []\n",
    "tools = [] \n",
    "paradigms = []\n",
    "platforms = []\n",
    "other = []\n",
    "frameworks = []\n",
    "storage = [] \n",
    "Education = []\n",
    "\n",
    "\n",
    "\n",
    "def Scraper(link):\n",
    "    driver.get(link)\n",
    "    \n",
    "    types = driver.find_elements_by_class_name('resume_top-tags')\n",
    "    \n",
    "    \n",
    "    # Name \n",
    "    name.append(driver.find_element_by_class_name('resume_top-info_name').text)\n",
    "    #Short description\n",
    "    short_description.append(driver.find_element_by_class_name('resume_top-info_short_description').text)\n",
    "    # Bio\n",
    "    bio.append(driver.find_element_by_class_name('resume_top-info_bio').text)\n",
    "    # Portfolio and Experience \n",
    "    portfolio_sec = driver.find_elements_by_class_name('resume_details-list_item')\n",
    "    \n",
    "    for i in portfolio_sec:\n",
    "        portfolio_and_experience.append(i.text)\n",
    "    \n",
    "    splitting_portfolio_and_experience = [i.splitlines() for i in portfolio_and_experience]\n",
    "    # Portfolio and Experience ----- Lists\n",
    "    portfolio.append(splitting_portfolio_and_experience[0:3])\n",
    "    experience.append(splitting_portfolio_and_experience[3:])\n",
    "    \n",
    "    \n",
    "    # Availability \n",
    "    availability = driver.find_element_by_xpath('//*[@id=\"Availability\"]/div/div/div[1]/div/div/h2').text\n",
    "    # Preferred_envioronment \n",
    "    preferred_environment.append(driver.find_element_by_xpath('//*[@id=\"Availability\"]/div/div/div[2]/div/div/div').text)\n",
    "    \n",
    "    \n",
    "    # Employment\n",
    "    employment_title1 = driver.find_elements_by_class_name('resume_section-content')\n",
    "    for i in employment_title1:\n",
    "        employment_title.append(i.text)\n",
    "    \n",
    "    \n",
    "    # Parsing the employment experience to create nested lists\n",
    "    parse = employment_title[0]\n",
    "\n",
    "    parsed = parse.splitlines()\n",
    "     \n",
    "    \n",
    "    for i in parsed:\n",
    "        if \"Technologies:\" in i:\n",
    "            techy.append(parsed.index(i))\n",
    "    \n",
    "    date_index = []\n",
    "    for i in parsed:\n",
    "        if re.match('[0-9/-]',i):\n",
    "            date_index.append(parsed.index(i))\n",
    "   \n",
    "    for i,t in zip(techy,date_index):\n",
    "        main_experience.append(parsed[t-1:i+1])\n",
    "        \n",
    "    # Languages\n",
    " \n",
    "    \n",
    "    lang = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[1]/div')\n",
    "    for i in lang:\n",
    "        languages.append(i.text)\n",
    "    \n",
    "    # Libraries and APIs   \n",
    "    \n",
    "    lib = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[2]/div')\n",
    "    for i in lib:\n",
    "        Libraries_APIs.append(i.text)\n",
    "        \n",
    " # tools  \n",
    "    \n",
    "    tool = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[3]/div')\n",
    "    \n",
    "    for i in tool:\n",
    "        tools.append(i.text)\n",
    "  \n",
    "  # Paradigms   \n",
    "    \n",
    "    \n",
    "    parad = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[4]/div')\n",
    "    \n",
    "    for i in parad:\n",
    "        paradigms.append(i.text)\n",
    "        \n",
    "   # Platforms \n",
    "    \n",
    "    platform = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[5]/div')\n",
    "    \n",
    "    for i in platform:\n",
    "        platforms.append(i.text)\n",
    "        \n",
    "    # Other\n",
    "    \n",
    "    others = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[6]/div')\n",
    "    for i in others:\n",
    "        other.append(i.text)\n",
    "        \n",
    "   # Frameworks \n",
    "    \n",
    "    framework = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[7]/div')\n",
    "    \n",
    "    for i in framework:\n",
    "        frameworks.append(i.text)\n",
    "    \n",
    "    # Storage\n",
    "    \n",
    "    storages = driver.find_elements_by_xpath('//*[@id=\"skills-list\"]/div/div/ul/li[8]/div')\n",
    "    \n",
    "    for i in storages:\n",
    "        storage.append(i.text)\n",
    "    \n",
    "    \n",
    "   # Education\n",
    "    \n",
    "    edu = driver.find_elements_by_xpath('//*[@id=\"EducationandCertifications\"]/div/div/div/div')\n",
    "    \n",
    "    for i in edu:\n",
    "        Education.append(i.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #portfolio_sec1 = portfolio_sec.find_elements_by_tag_name('a')                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "name.clear()\n",
    "short_description.clear()\n",
    "bio.clear()\n",
    "availability.clear()\n",
    "preferred_environment.clear() \n",
    "portfolio_and_experience.clear()\n",
    "portfolio.clear()\n",
    "experience.clear() \n",
    "employment_title.clear()\n",
    "employment_company.clear() \n",
    "employment_dates.clear()\n",
    "employment_technologies.clear()\n",
    "main_experience.clear()\n",
    "date_index.clear()\n",
    "techy.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self Employed\n",
      "2018 - PRESENT\n",
      "Independent Machine Learning Consultant\n",
      "Built and deployed multiple personalization ML pipelines to lift offer/coupon conversion rate for customers of major restaurant chain. Application built on Azure Databricks platform with business-configurable pipelines for training, tuning, testing and prediction, using Pandas (Python), Spark (PySpark), sklearn and Spark ML. Deployment to production using Azure Data Factory.\n",
      "Automated, hardened, and deployed multiple ML pipelines on AWS (Elastic Map Reduce with Spark and Lambda) to predict next-best-action, forecast performance and predict/prevent churn for sales representatives of major corporation. Data processing used Python, PySpark and Spark SQL. ML models built using Microsoft ML for Spark.\n",
      "Advised a mid-stage startup on the requirements, features, and architecture needed to support ML pipelines in their high-speed stream processing framework and in-memory data grid. Worked directly with the CEO/CTO and senior technical team.\n",
      "Technologies: Optimization, AWS S3, AWS EC2, Docker, Python, Pandas, Scikit-learn, XGBoost, Solutions Architecture, System Architecture\n",
      "Senior Product Architect, Infosys Nia (Palo Alto)\n",
      "2017 - 2018\n",
      "Infosys Technologies\n",
      "Developed and prioritized the roadmap for the integration of Skytree software into Infosys Nia.\n",
      "Trained 100+ Infosys sales leaders, solutions architects and data scientists on Skytree capabilities, technology, architecture, system requirements, demos etc. Also trained enterprise-wide data science teams on ML science and best practices.\n",
      "Evangelized the newly acquired ML capabilities to Fortune 500 prospects as well as existing clients.\n",
      "Technologies: AWS EC2, AWS EMR, AWS S3, HDFS, MapReduce, YARN, Hadoop, Scikit-learn, Pandas, NumPy, SciPy, Python, R, Linux, Bash, Java, OpenMP, MPI, C++, Machine Learning\n",
      "Co-Founder\n",
      "2009 - 2017\n",
      "Skytree Inc.\n",
      "Worked directly with our Fortune 500 customers and collaboratively built predictive machine learning models/pipelines for fraud detection (for American Express), product and media recommender systems (for Samsung), credit risk scoring - consumer and SMB (for American Express and Equifax), Lead Scoring - Premium Consumer Credit Card (for American Express), Balance Transfer Offer Optimization (for Discover), churn prevention (E-Harmony, ShoeDazzle), real estate price prediction (Brookfield RPS), and many others for Fortune 500 clients.\n",
      "Led engineering and data science and ultimately moved to technical product management and ownership for Skytree’s flagship product. Led the research and development of Skytree’s high performance and massively parallel C++ library for tera-scale ML. Implemented (from scratch) mathematically scalable and distributed algorithms for nearest neighbors, random forests, gradient boosted trees, support vector machines, clustering, collaborative filtering, etc. for classification, regression, anomaly detection, and recommender systems. This included many first of the kind innovations in the practical application of ML algorithms to big data.\n",
      "Architected Skytree’s (flagship) Infinity AI platform, including APIs, GUI, and SDKs. The Java-based server coordinated with the underlying multi-tenant Big Data or cloud infrastructure, managing data, users, resources, and scheduling jobs (a mix of Apache Spark for data processing and Skytree’s C++ engine for ML). Platform support included Apache Hadoop (YARN & HDFS) from MapR, Hortonworks, and Cloudera as well as AWS Elastic Map Reduce.\n",
      "Delivered multiple releases of the full stack of Skytree’s AI software as the product manager for all four technical teams (ML, systems, UI, and data science), including defining and prioritizing the roadmap and coordinating release and development efforts across teams.\n",
      "Built world-class engineering (C++/HPC/ML, Java/Systems, and UI) and data science team. Defined requirements, developed and reviewed screening tests, and finalized candidates.\n",
      "Spearheaded the technical sales enablement efforts.\n",
      "Supported POCs, pre and post-sales activities, renewals, through product demos, sales calls, requirements gathering, trade shows, webinars, seminars, and tutorials.\n",
      "Trained solutions architects/sales engineers and had ownership of the technical resources they needed (demos, documentation, guides, questionnaires, etc.).\n",
      "Co-authored five patent applications in the areas of ML user experience, recommender systems, and automatic feature engineering.\n",
      "Recruited candidates for various other positions, from sales directors to senior leadership (VP of sales, marketing, and engineering).\n",
      "Technologies: Linux, Bash, Java, R, Pandas, NumPy, SciPy, Scikit-learn, Python, AWS EC2, AWS EMR, AWS S3, HDFS, MapReduce, YARN, Hadoop, Apache Spark, OpenMP, MPI, C++\n",
      "Graduate Research Assistant\n",
      "2007 - 2009\n",
      "Georgia Institute of Technology\n",
      "Worked on integrating algorithmically optimized machine learning algorithms directly into SQL Server using the .NET platform and C# so that they ran natively inside the database under the purview of the database scheduler.\n",
      "Designed innovative disk-based algorithms to piggyback multi-dimensional space trees over database indexes (B-Tree's) to minimize disk hit rate and optimize cash hit ratio.\n",
      "Specialized in computational science and engineering, high-performance computing, and artificial intelligence.\n",
      "Technologies: .NET, Microsoft, Microsoft SQL Server, Java, C#\n",
      "Software Developer (Intern), Analysis Services, SQL Server Team\n",
      "2008 - 2008\n",
      "Microsoft\n",
      "Integrated advanced ML algorithms, optimized for disk-based I/O, as first-class objects into SQL Server Analysis Services and exposed these through the query interface- thus enabling ML models to run in-database.\n",
      "Technologies: C#, Microsoft SQL Server\n",
      "Technical Associate\n",
      "2005 - 2007\n",
      "Trilogy\n",
      "Designed and developed the software for Trilogy's email marketing service for, used by clients such as Gateway and Orbitz. The software used segmentation and association rule mining to increase sales, margins, and engagement (email opens and clicks), and integrated data such as demographic, email activity, clickstream, promotional, etc.\n",
      "Executed weekly campaigns that generated millions of targeted emails, measured lift through A/B testing and reported results in the form of pivot tables and dashboards.\n",
      "Technologies: Microsoft SQL Server, Subversion (SVN), Microsoft, Java\n"
     ]
    }
   ],
   "source": [
    "# print(Ai.name)\n",
    "# print(Ai.short_description)\n",
    "# print(Ai.bio)\n",
    "# print(Ai.preferred_environment)\n",
    "\n",
    "# print(Ai.portfolio)\n",
    "# print(Ai.experience)\n",
    "\n",
    "print(Ai.employment_title[0])\n",
    "\n",
    "\n",
    "# Portfolio and Experience \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = []\n",
    "for i in parsed:\n",
    "    if re.match('[0-9/-]',i):\n",
    "        date_index.append(parsed.index(i))\n",
    "main_experience = []        \n",
    "for i,t in zip(techy,date_index):\n",
    "    main_experience.append(parsed[t-1:i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = []\n",
    "# date = []\n",
    "# company = []\n",
    "# tech = []\n",
    "# experience = []\n",
    "# index = []\n",
    "\n",
    "# pattern = re.compile('[0-9/-]')\n",
    "\n",
    "# for i in parsed:\n",
    "# #Date \n",
    "#     if re.match('[0-9/-]',i):\n",
    "#         date.append(i)\n",
    "#         index.append(parsed.index(i))\n",
    "# # experience \n",
    "#     elif len(i) >50:\n",
    "#         experience.append(i)   \n",
    "# #technologies \n",
    "#     elif \"Technologies:\" in i:\n",
    "#         tech.append(i)\n",
    "        \n",
    "# # Title         \n",
    "# for i in index:\n",
    "#     title.append(parsed[i-1])\n",
    "        \n",
    "        \n",
    "    \n",
    "#     #Technologies\n",
    "  \n",
    "        \n",
    "# #for i,line in enumerate(parsed):\n",
    "#    # print(i,line)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
